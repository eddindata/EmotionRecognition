{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
    "from keras.models import Model,Sequential\n",
    "from keras.optimizers import Adam,SGD,RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = 'data_color/'\n",
    "picture_size = 96\n",
    "batch_size  = 64\n",
    "var_seed = 17\n",
    "no_of_classes = 8\n",
    "epochs = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23237 images belonging to 8 classes.\n",
      "Found 5805 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen_train = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255, validation_split=0.2)\n",
    "\n",
    "train_set = datagen_train.flow_from_directory(#dataPath+\"train\",\n",
    "                                            dataPath,\n",
    "                                            subset=\"training\",\n",
    "                                            target_size = (picture_size,picture_size),\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode = 'categorical',\n",
    "                                            shuffle = True, \n",
    "                                            seed  = var_seed)\n",
    "\n",
    "val_set = datagen_train.flow_from_directory(#dataPath+\"test\",\n",
    "                                            dataPath,\n",
    "                                            subset=\"validation\",\n",
    "                                            target_size = (picture_size,picture_size),\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode = 'categorical',\n",
    "                                            shuffle = False, \n",
    "                                            seed  = var_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vgg16 color model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 3, 3, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               2359808   \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 4104      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17080648 (65.16 MB)\n",
      "Trainable params: 2364936 (9.02 MB)\n",
      "Non-trainable params: 14715712 (56.14 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16 = tf.keras.applications.VGG16(input_shape=(picture_size, picture_size, 3), include_top=False, weights='imagenet')\n",
    "vgg16.trainable = False\n",
    "\n",
    "vgg16_model = tf.keras.Sequential([\n",
    "    vgg16,\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dense(512),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    tf.keras.layers.Dense(no_of_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = Adam(learning_rate= 0.0001)\n",
    "vgg16_model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"models/color/vgg16_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0.0001,\n",
    "                          patience=6,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_learningrate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=6,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "callbacks_list = [early_stopping,checkpoint,reduce_learningrate]\n",
    "\n",
    "vgg16_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 1.6397 - accuracy: 0.4077\n",
      "Epoch 1: val_accuracy improved from -inf to 0.39862, saving model to models/color\\vgg16_model.h5\n",
      "364/364 [==============================] - 347s 951ms/step - loss: 1.6397 - accuracy: 0.4077 - val_loss: 1.9936 - val_accuracy: 0.3986 - lr: 0.0010\n",
      "Epoch 2/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Estudios\\MCDD\\Mod_12_DeepLearning\\TrabajoFinal\\Proyecto\\env\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - ETA: 0s - loss: 1.2449 - accuracy: 0.5389\n",
      "Epoch 2: val_accuracy improved from 0.39862 to 0.41809, saving model to models/color\\vgg16_model.h5\n",
      "364/364 [==============================] - 332s 913ms/step - loss: 1.2449 - accuracy: 0.5389 - val_loss: 2.1081 - val_accuracy: 0.4181 - lr: 0.0010\n",
      "Epoch 3/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 1.0970 - accuracy: 0.5943\n",
      "Epoch 3: val_accuracy did not improve from 0.41809\n",
      "364/364 [==============================] - 334s 918ms/step - loss: 1.0970 - accuracy: 0.5943 - val_loss: 1.8139 - val_accuracy: 0.3811 - lr: 0.0010\n",
      "Epoch 4/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 0.9724 - accuracy: 0.6448\n",
      "Epoch 4: val_accuracy improved from 0.41809 to 0.43376, saving model to models/color\\vgg16_model.h5\n",
      "364/364 [==============================] - 333s 914ms/step - loss: 0.9724 - accuracy: 0.6448 - val_loss: 1.8820 - val_accuracy: 0.4338 - lr: 0.0010\n",
      "Epoch 5/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 0.8509 - accuracy: 0.6895\n",
      "Epoch 5: val_accuracy did not improve from 0.43376\n",
      "364/364 [==============================] - 333s 916ms/step - loss: 0.8509 - accuracy: 0.6895 - val_loss: 1.9225 - val_accuracy: 0.4300 - lr: 0.0010\n",
      "Epoch 6/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 0.7552 - accuracy: 0.7305\n",
      "Epoch 6: val_accuracy did not improve from 0.43376\n",
      "364/364 [==============================] - 333s 914ms/step - loss: 0.7552 - accuracy: 0.7305 - val_loss: 2.3263 - val_accuracy: 0.4300 - lr: 0.0010\n",
      "Epoch 7/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 0.6462 - accuracy: 0.7715\n",
      "Epoch 7: val_accuracy improved from 0.43376 to 0.43428, saving model to models/color\\vgg16_model.h5\n",
      "364/364 [==============================] - 334s 917ms/step - loss: 0.6462 - accuracy: 0.7715 - val_loss: 2.1869 - val_accuracy: 0.4343 - lr: 0.0010\n",
      "Epoch 8/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 0.5536 - accuracy: 0.8056\n",
      "Epoch 8: val_accuracy did not improve from 0.43428\n",
      "364/364 [==============================] - 332s 912ms/step - loss: 0.5536 - accuracy: 0.8056 - val_loss: 2.3897 - val_accuracy: 0.4176 - lr: 0.0010\n",
      "Epoch 9/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 0.5112 - accuracy: 0.8236Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.43428\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "364/364 [==============================] - 333s 916ms/step - loss: 0.5112 - accuracy: 0.8236 - val_loss: 2.4205 - val_accuracy: 0.4331 - lr: 0.0010\n",
      "Epoch 9: early stopping\n",
      "CPU times: total: 5h 28min 40s\n",
      "Wall time: 50min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = vgg16_model.fit(train_set,\n",
    "                                epochs=epochs,\n",
    "                                validation_data = val_set,\n",
    "                                callbacks=callbacks_list\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 3, 3, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 18432)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               4718848   \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 256)               1024      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 256)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 2056      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28309640 (107.99 MB)\n",
      "Trainable params: 4721416 (18.01 MB)\n",
      "Non-trainable params: 23588224 (89.98 MB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "resnet = tf.keras.applications.ResNet50(input_shape=(picture_size, picture_size, 3), include_top=False, weights='imagenet')\n",
    "resnet.trainable = False\n",
    "\n",
    "resnet_model = tf.keras.Sequential([\n",
    "    resnet,\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(256),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "\n",
    "    tf.keras.layers.Dense(no_of_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = Adam(learning_rate= 0.0001)\n",
    "resnet_model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"models/color/resnet_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0.0001,\n",
    "                          patience=6,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_learningrate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=6,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "callbacks_list = [early_stopping,checkpoint,reduce_learningrate]\n",
    "\n",
    "resnet_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 1.6282 - accuracy: 0.4019\n",
      "Epoch 1: val_accuracy improved from -inf to 0.42825, saving model to models/color\\resnet_model.h5\n",
      "364/364 [==============================] - 327s 897ms/step - loss: 1.6282 - accuracy: 0.4019 - val_loss: 1.7271 - val_accuracy: 0.4283 - lr: 0.0010\n",
      "Epoch 2/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 1.2788 - accuracy: 0.5243\n",
      "Epoch 2: val_accuracy did not improve from 0.42825\n",
      "364/364 [==============================] - 327s 898ms/step - loss: 1.2788 - accuracy: 0.5243 - val_loss: 1.7655 - val_accuracy: 0.4167 - lr: 0.0010\n",
      "Epoch 3/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 1.1419 - accuracy: 0.5751\n",
      "Epoch 3: val_accuracy improved from 0.42825 to 0.42963, saving model to models/color\\resnet_model.h5\n",
      "364/364 [==============================] - 328s 901ms/step - loss: 1.1419 - accuracy: 0.5751 - val_loss: 1.8244 - val_accuracy: 0.4296 - lr: 0.0010\n",
      "Epoch 4/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 1.0345 - accuracy: 0.6173\n",
      "Epoch 4: val_accuracy improved from 0.42963 to 0.43359, saving model to models/color\\resnet_model.h5\n",
      "364/364 [==============================] - 328s 901ms/step - loss: 1.0345 - accuracy: 0.6173 - val_loss: 1.8288 - val_accuracy: 0.4336 - lr: 0.0010\n",
      "Epoch 5/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 0.9319 - accuracy: 0.6608\n",
      "Epoch 5: val_accuracy improved from 0.43359 to 0.43463, saving model to models/color\\resnet_model.h5\n",
      "364/364 [==============================] - 329s 904ms/step - loss: 0.9319 - accuracy: 0.6608 - val_loss: 2.0394 - val_accuracy: 0.4346 - lr: 0.0010\n",
      "Epoch 6/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 0.8566 - accuracy: 0.6871\n",
      "Epoch 6: val_accuracy did not improve from 0.43463\n",
      "364/364 [==============================] - 327s 900ms/step - loss: 0.8566 - accuracy: 0.6871 - val_loss: 2.0073 - val_accuracy: 0.4258 - lr: 0.0010\n",
      "Epoch 7/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 0.7618 - accuracy: 0.7263Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.43463\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "364/364 [==============================] - 328s 902ms/step - loss: 0.7618 - accuracy: 0.7263 - val_loss: 2.0825 - val_accuracy: 0.4234 - lr: 0.0010\n",
      "Epoch 7: early stopping\n",
      "CPU times: total: 4h 11min 24s\n",
      "Wall time: 38min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = resnet_model.fit(train_set,\n",
    "                                epochs=epochs,\n",
    "                                validation_data = val_set,\n",
    "                                callbacks=callbacks_list\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 96, 96, 64)        1792      \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 96, 96, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 96, 96, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 48, 48, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 48, 48, 64)        0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 48, 48, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 48, 48, 128)       512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 48, 48, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 24, 24, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 24, 24, 128)       0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 24, 24, 128)       512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 24, 24, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 12, 12, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 12, 12, 256)       1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 12, 12, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 6, 6, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 6, 6, 256)         1638656   \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 6, 6, 256)         1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPooli  (None, 3, 3, 256)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 3, 3, 512)         3277312   \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 3, 3, 512)         2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPooli  (None, 1, 1, 512)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 512)               2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 8)                 4104      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5708552 (21.78 MB)\n",
      "Trainable params: 5704840 (21.76 MB)\n",
      "Non-trainable params: 3712 (14.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "own_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64,(3,3),padding = 'same',input_shape = (96,96,3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),   \n",
    "    tf.keras.layers.Dropout(0.25),   \n",
    "    \n",
    "    \n",
    "    tf.keras.layers.Conv2D(128,(3,3),padding = 'same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),   \n",
    "    tf.keras.layers.Dropout(0.25),    \n",
    "\n",
    "    tf.keras.layers.Conv2D(128,(3,3),padding = 'same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),   \n",
    "    tf.keras.layers.Dropout(0.25),       \n",
    "\n",
    "    tf.keras.layers.Conv2D(256,(3,3),padding = 'same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),   \n",
    "    tf.keras.layers.Dropout(0.30),  \n",
    "    \n",
    "    tf.keras.layers.Conv2D(256,(5,5),padding = 'same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),   \n",
    "    tf.keras.layers.Dropout(0.30),     \n",
    "    \n",
    "    tf.keras.layers.Conv2D(512,(5,5),padding = 'same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size = (2,2)),   \n",
    "    tf.keras.layers.Dropout(0.30), \n",
    "       \n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dense(512),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    tf.keras.layers.Dense(no_of_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = Adam(learning_rate= 0.0001)\n",
    "own_model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "own_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"models/color/own_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0.001,\n",
    "                          patience=6,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_learningrate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=6,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.001)\n",
    "\n",
    "callbacks_list = [early_stopping,checkpoint,reduce_learningrate]\n",
    "\n",
    "\n",
    "own_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 1.6295 - accuracy: 0.3515\n",
      "Epoch 1: val_accuracy improved from -inf to 0.35211, saving model to models/color\\own_model.h5\n",
      "364/364 [==============================] - 492s 1s/step - loss: 1.6295 - accuracy: 0.3515 - val_loss: 2.1930 - val_accuracy: 0.3521 - lr: 0.0010\n",
      "Epoch 2/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 1.1991 - accuracy: 0.5177\n",
      "Epoch 2: val_accuracy improved from 0.35211 to 0.49078, saving model to models/color\\own_model.h5\n",
      "364/364 [==============================] - 491s 1s/step - loss: 1.1991 - accuracy: 0.5177 - val_loss: 2.3147 - val_accuracy: 0.4908 - lr: 0.0010\n",
      "Epoch 3/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 1.0794 - accuracy: 0.5703\n",
      "Epoch 3: val_accuracy improved from 0.49078 to 0.52730, saving model to models/color\\own_model.h5\n",
      "364/364 [==============================] - 492s 1s/step - loss: 1.0794 - accuracy: 0.5703 - val_loss: 2.3338 - val_accuracy: 0.5273 - lr: 0.0010\n",
      "Epoch 4/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 1.0318 - accuracy: 0.5938\n",
      "Epoch 4: val_accuracy did not improve from 0.52730\n",
      "364/364 [==============================] - 491s 1s/step - loss: 1.0318 - accuracy: 0.5938 - val_loss: 1.8704 - val_accuracy: 0.4873 - lr: 0.0010\n",
      "Epoch 5/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 0.9676 - accuracy: 0.6237\n",
      "Epoch 5: val_accuracy improved from 0.52730 to 0.55538, saving model to models/color\\own_model.h5\n",
      "364/364 [==============================] - 489s 1s/step - loss: 0.9676 - accuracy: 0.6237 - val_loss: 2.1853 - val_accuracy: 0.5554 - lr: 0.0010\n",
      "Epoch 6/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 0.9354 - accuracy: 0.6396\n",
      "Epoch 6: val_accuracy did not improve from 0.55538\n",
      "364/364 [==============================] - 491s 1s/step - loss: 0.9354 - accuracy: 0.6396 - val_loss: 1.9811 - val_accuracy: 0.5442 - lr: 0.0010\n",
      "Epoch 7/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 0.8986 - accuracy: 0.6553\n",
      "Epoch 7: val_accuracy improved from 0.55538 to 0.59535, saving model to models/color\\own_model.h5\n",
      "364/364 [==============================] - 490s 1s/step - loss: 0.8986 - accuracy: 0.6553 - val_loss: 2.4493 - val_accuracy: 0.5953 - lr: 0.0010\n",
      "Epoch 8/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 0.8666 - accuracy: 0.6701\n",
      "Epoch 8: val_accuracy did not improve from 0.59535\n",
      "364/364 [==============================] - 488s 1s/step - loss: 0.8666 - accuracy: 0.6701 - val_loss: 2.0419 - val_accuracy: 0.5640 - lr: 0.0010\n",
      "Epoch 9/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 0.8362 - accuracy: 0.6835\n",
      "Epoch 9: val_accuracy did not improve from 0.59535\n",
      "364/364 [==============================] - 487s 1s/step - loss: 0.8362 - accuracy: 0.6835 - val_loss: 2.3456 - val_accuracy: 0.5602 - lr: 0.0010\n",
      "Epoch 10/75\n",
      "364/364 [==============================] - ETA: 0s - loss: 0.8096 - accuracy: 0.6939Restoring model weights from the end of the best epoch: 4.\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.59535\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "364/364 [==============================] - 489s 1s/step - loss: 0.8096 - accuracy: 0.6939 - val_loss: 2.2766 - val_accuracy: 0.5523 - lr: 0.0010\n",
      "Epoch 10: early stopping\n",
      "CPU times: total: 4h 3min 17s\n",
      "Wall time: 1h 21min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = own_model.fit(train_set,\n",
    "                                epochs=epochs,s\n",
    "                                validation_data = val_set,\n",
    "                                callbacks=callbacks_list\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
